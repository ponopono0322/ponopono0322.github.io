---
title:  "[ML]제주도 여행지 추천 모델 제작-5"
excerpt: "Word2Vec"

categories:
  - ML
tags:
  - [ML, KoNLPy, Cosine similarity, Max score tokenizer, Word2Vec, Soynlp]

toc: true
toc_sticky: true
 
date: 2021-08-30
last_modified_at: 2021-08-30
---
# 시작하기 앞서
## 지난 포스트 요약
1. KoNLPy 라이브러리 사용
2. 코사인 유사도 측정

## 이번 포스트에서는
정돈된 데이터로 Word2Vec을 사용해보고, 그 결과를 어떻게 응용했는지를 다룹니다.

1. 어떤 라이브러리를 사용했는가?
2. Word2Vec 사용하기
3. Word2Vec 결과를 응용하기
4. 인스타그램 해시태그를 적용하기
5. 음식점 태그를 적용하기
6. 숙소에도 적용해보기
7. 정리
8. 마치며

# 1. 어떤 라이브러리를 사용했는가?
이번 포스트에서 사용한 라이브러리는 다음과 같습니다.
1. Scikit-learn
2. Pandas
3. Numpy
4. Difflib

# 2. Word2Vec 사용하기
## 경험적 수치조정
Word2Vec의 파라미터로 넣었던 부분은 다음과 같다.
1. vector_size      //행렬크기
2. window           //앞뒤로 볼 단어 범위
3. min_count        //최소 등장 수
4. sg               //CBOW 또는 Skip-gram

일반적으로 vector_size는 짧은 문장의 경우 100 ~ 300, 긴 문장의 경우 500 ~ 1000 사이로 넣는다고 한다. 우리의 경우 한 문장의 길이가 길지 않았으므로 100 ~ 300 사이의 숫자를 선택할 필요가 있었고, 동일 조건아래 vector_size만 변경시켜 구동해본 결과, 400이상을 주게 되면 결과값 차이는 미미하고 시간만 오래 걸렸고, 200이하로 주기에는 성능 하락이 눈에 띄게 보여 300으로 설정하게 됐다.

window의 경우 애초에 추출된 명사 갯수 자체가 보통 10개 안쪽이라 크게 넓힐 필요가 없었다. 앞뒤로 5개, 총 10개로 해도 결과 차이가 없었으므로 window는 5로 주었다.

min_count는 1번만 등장하게 하면 이상한 단어들이 많이 포함됐던 탓에 1 보다는 큰 값이 필요했고, 그렇다고 값을 5이상 주게 되면, 생략되는 단어가 많아져 적당한 해시태그 뽑기가 어려웠다. 따라서 이 부분은 2로 주게 되었다.

sg는 CBOW = 0, Skip-gram = 1으로 정해졌기 때문에 고민없이 1로 설정하였다.

worker는 사용할 코어 수다. 자신의 컴퓨터 성능에 맞게 조절하면 되겠다.

```python
from gensim.models import Word2Vec
model = Word2Vec(result, vector_size=300, window=5, min_count=2, workers=12, sg=1)
#smilarity는 앞의 단어와 뒤의 단어의 유사도를 숫자로 보여주는 함수다
model_result = model.wv.similarity('우도', '부모님')
print(model_result)
#most_similar는 단어와 가장 유사한 단어들을 보여주는 함수다.
model_result1 = model.wv.most_similar('우도')
print(model_result1)
```

## 해시태그와 결합하기
Word2Vec의 similarity 함수를 사용하면 장소와 해시태그 유사도를 알아낼 수 있을 것 같았다. 장소와 해시태그의 유사도가 높을수록 개인에게 그 장소가 적합하다고 판단한 것이다. 그렇기 때문에 해시태그만 정리하면 되었다.

그래서 우선적으로 이미 있는 해시태그를 쓰기로 했다. 비짓제주의 정돈된 해시태그를 우선적으로 적용한 후 다른 키워드들을 적용하면 될 것 같았다.   
비짓제주에서 제공하는 키워드는 다음과 같았다.   
1. 일행: 부모, 친구, 커플, 혼자, 아이
2. 취향: 경관/포토, 휴식/힐링, 테마공원, 문화유적지, 미술/박물관, 엑티비티, 체험관광, 드라이브, 쇼핑
3. 날씨: 비/눈, 흐림, 맑음
4. 계절: 봄, 여름, 가을, 겨울, 사계절

그 중 우리는 1, 2번의 내용만 쓰기로 했다.

우선, 관광지 목록을 분석기에 넣고 명사만 추출해, 핵심 단어만 뺐다. 이 작업을 한 이유는 okt분석기로 관광지 이름도 나눠지기 때문에 문장 그대로 넣게되면 단어를 찾을 수 없다고 오류가 나기 때문이다.

```python
n_data = pd.read_csv('./크롤링 데이터/관광지목록.csv', encoding='cp949')
from konlpy.tag import Okt
okt=Okt()
result = []

for i in n_data.name:
    tokenlist = okt.pos(i, stem=True, norm=True)
    temp=[]
    for word in tokenlist:
        if word[1] in ["Noun"]: # 명사일 때만
            temp.append((word[0])) # 해당 단어를 저장함
    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만
        result.append(temp) # 결과에 저장
    else:
        result.append(i)
```

이렇게 나눈 이름 중에 '제주'라는 단어로 시작하는 경우가 많아 이들은 뒤에 있는 단어로 확인할 필요가 있어 또 한 번 정제했다.

```python

u_list = []
num=0
for i in result:
    try:
        if i[0] == '제주':
            u_list.append(i[1])
        else:
            if i[0] in u_list:
                if len(i[1]) == 1:
                    u_list.append(n_data.name[num][:2])
                else:
                    u_list.append(i[1])
            else:
                if len(i[0]) == 1:
                    u_list.append(n_data.name[num][:2])
                else:
                    u_list.append(i[0])
    except:
        u_list.append(n_data.name[num][:2])
    num+=1
```

그렇게 나눈 이름들을 similarity 함수에 적용했다.

```python
def dic_maker(list1, list2, list3, list4, name):
    zip_list = {}
    zip_list['장소'] = name
    for i,j in zip(list1, list2):
        zip_list[i] = j
    for i,j in zip(list3, list4):
        zip_list[i] = j
        
    return zip_list

who_with = ['부모님','아이','친구','커플',
            '친척','가족','지인','연인',
            '혼자','둘','셋','넷']

taste = ['경관','인생샷','휴식','공원','유적지','미술','박물관',
         '익스트림','스포츠','체험','드라이브','쇼핑','전시','자연','산책']

room = ['안전인증민박','굿스테이','관광호텔','전통/가족호텔','호스텔/소형호텔',
        '휴양펜션','휴양콘도','일반숙박','생활숙박','농어촌민박','유스호스텔']

r_df = pd.DataFrame()

result = np.load('./list/word_noun_list.npy', allow_pickle=True)
print(result)

from gensim.models import Word2Vec
model = Word2Vec(result, vector_size=300, window=5, min_count=2, workers=8, sg=1, epochs=5)

for name in u_list:
    w_list = []
    t_list = []
    try:
        for para in who_with:
            sim = model.wv.similarity(name, para)
            w_list.append(sim)

        for para in taste:
            sim = model.wv.similarity(name, para)
            t_list.append(sim)

    except:
        print(name)
    r_df = r_df.append(dic_maker(who_with, w_list, taste, t_list, name), ignore_index=True)
```

## 유사도 측정된 데이터프레임을 정규화 하기
워낙 날 것 상태의 데이터들을 한번에 돌리다보니 사실 유사도 차이가 그렇게 심하지 못했다. 대부분의 데이터가 0.5 이하의 유사성을 갖게 되고, 주로 0.2 ~ 0.3 사이에 쏠려있음을 확인했다.

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(data)
data_scaled = scaler.transform(data)
data_df_scaled = pd.DataFrame(data=data_scaled, columns=data.columns, index=data.index)
data_df_scaled
```

MinMaxScaler를 통해 정규화를 진행했고, 예시를 들기 위해 전체데이터 기반으로 x축은 부모님, y축은 혼자 라는 라벨을 사용했고, 전후 차이는 다음과 같았다.

<img width="724" alt="캡처" src="https://user-images.githubusercontent.com/32767165/131298451-058cbde1-508e-41a2-a363-18e633f42529.PNG">

0.5 이하였던 데이터들이 1이하로 비교적 넓게 분포됨을 확인 할 수 있었다.

## 의사결정트리에 넣을 순 없을까?
프로젝트 초기부터 들었던 생각은 "개인 성향대로 맞다 아니다, ~보다 크다 작다로 쪼개들어가다보면 적당한 장소가 나오겠지." 였다. 그래서 이렇게 숫자로 된 데이터를 의사결정트리에 넣어보게 되었다.

<img width="730" alt="1" src="https://user-images.githubusercontent.com/32767165/131299353-bae6c77f-de9f-406a-8739-dc9820cb5b0e.PNG">

결과는 매우 충격적이였다. 정확도가 30% 정도밖에 안된다니. 처음에는 코드를 잘못 짠 줄 알았다. 급하게 아무거나 긁어와서 그랬겠지. 라고 생각했으나, 그 예상은 보기좋게 빗나갔다.

## 절대적으로 부족한 데이터
일단 다시 생각해보기로 했다. 흔히 머신러닝 입문용 데이터셋으로 쓰는 붓꽃데이터를 어떻게 썼는지 곰곰히 떠올려봤다. 데이터 수는 몇천, 몇 만개, 그리고 정답레이블은 3개다. 반면 우리의 데이터는 레이블 1000개 이상, 데이터 수는 레이블 당 한개다. 즉, 우리가 갖고 있던 데이터가 많아보이긴 해도 사실, 머신러닝을 돌리기 위해 필요한 실제 데이터는 턱없이 부족했던 것이다.

해결방안으로 Word2Vec의 파라미터의 값을 변경하면서 해보기도 했지만, 결국 데이터 부풀리기에 그치는 것 같아(한 레이블당 27개로 부풀려서 한 결과 정확도는 48%까지 올라가긴 했다) 그만하기로 했다.

# 3. Word2Vec 결과를 응용하기
그래서 다음 대안으로 생각한 것은 정렬이였다. 가중치는 사용자 딴에서 정렬하기로 하고, 그 정렬된 해시태그 순으로 데이터를 정렬해보기로 했다.   
정렬된 데이터 중 사용자가 선택한 해시태그만 평균을 내어 가장 높은 값을 사용자가 원한 결과라고 가정했다.

```python
finder = ['연인','둘','드라이브','산책'] #사용자가 원하는 해시태그라 가정
f_data = data_df_scaled.sort_values(by=finder, ascending=False) #그 순서대로 정렬
pure_name = f_data[finder].mean(axis=1).sort_values(ascending=False).head(10).index.to_list() #고른 해시태그만 평균내기
```

이렇게 나온 데이터를 관광지 이름과 비교하여 매칭되는 장소를 골라냈다.

```python
all_data = pd.read_excel('./list/수정된파일.xlsx')
all_data.drop('Column1', axis=1, inplace=True)

only_tour = all_data[all_data['타입']=='관광지']
only_tour = only_tour.loc[:,'타입':'기타 상세']

pure_df = pd.DataFrame()
for name in pure_name:   
    df = only_tour[only_tour['장소'].str.contains(name)]
    pure_df = pure_df.append(df)
```

# 4. 인스타그램 해시태그를 적용하기
위에서 정리된 관광지 데이터를 사용자에게 출력하기 직전에 내부적인 가중치 조정을 한 뒤 출력하기로 했다.    
그 중 인스타그램 해시태그 게시물 수와 비짓제주의 좋아요, 찜, 조회수를 추가하기로 했다.

그러기 위해선 한 장소에 대한 인스타그램의 해시태그들을 합칠 필요가 있었다.

```python
abc = pd.DataFrame()

for i in tops.장소:
    co = 0
    for j in hash_tag.hashtag:
        if SequenceMatcher(None, i, j).ratio() > 0.6:
            co += hash_tag[hash_tag.hashtag == j]['count'].values
    abc = abc.append({'name':i,'count':int(co)}, ignore_index=True)
abc = abc.astype({'count':int})

final = pd.merge(tops, abc, right_on='name', left_on='장소').drop('name', axis=1)
final = final.sort_values('count', ascending=False)
final
```

위의 코드에서 보이듯이 문자열의 일치율을 확인하기 위한 라이브러리를 사용했다. 수치를 0.5로 조정할 때 '제주'라는 단어 때문에 일치되는 여행지가 많아 경험적 수치조정을 통해 0.6이 가장 적합하다고 판단, 적용했다.    
그렇게 조건에 맞는 데이터의 게시물 수를 합해서 사용자에게 보여줄 데이터프레임에 열추가를 통해 카운트를 했다.

```python
def des(a,b,c,d):
    return a + b*10 + c*0.1 + d
final['가중치'] = final.apply(lambda df: des(df.좋아요, df.찜하기, df.조회, df['count']), axis=1)
final.drop(['좋아요','찜하기','조회','count'], axis=1, inplace=True)

final.drop_duplicates(['장소'], keep='first', inplace=True)
final.set_index('장소', inplace=True)
final.sort_values('가중치', ascending=False)
```

위의 코드는 추가된 count 열과 비짓제주의 좋아요, 찜, 조회수를 나름의 함수를 통해 가중치를 부여해 계산하고, 그 값을 가중치라는 새로운 열로 지정해 넣었다.

# 5. 음식점 태그를 적용하기
## 기존 데이터 수정하기
우리가 이전에 비짓제주에서 크롤링한 데이터를 재가공할 필요가 있어 선제적 조치를 취할 필요가 있었다.

```python
def to_string(arr):
    ml = arr.replace(' ','').split("'")
    if '[' in ml:
        ml.remove('[')
    if ']' in ml:
        ml.remove(']')
    while ',' in ml:
        ml.remove(',')
    string = ''
    for i in ml:
        string = string+' '+i
    return string

def to_list(arr):
    ml = arr.replace(' ','').split("'")
    if '[' in ml:
        ml.remove('[')
    if ']' in ml:
        ml.remove(']')
    while ',' in ml:
        ml.remove(',')
    return ml
```

정의한 함수를 다음과 같이 원하는 형태로 변환하는 작업을 거쳤다.

```python
#문자열의 형태인 데이터를 배열형태로 바꾸는 작업
final['관광지'] = final.apply(lambda val:to_list(val.관광지), axis=1)
final['음식점'] = final.apply(lambda val:to_list(val.음식점), axis=1)
final['숙박'] = final.apply(lambda val:to_list(val.숙박), axis=1)

l_data = final['음식점'][0] #사용자와 가장 매칭이 잘 되는 관광지의 음식점 목록

df = pd.read_excel('./list/수정된파일.xlsx')
df.drop('Column1', axis=1, inplace=True)
df = df[df['타입'] == '음식점']

#띄어쓰기로 되어 있어야 코사인 유사도 측정이 가능하기 때문에 문자열 데이터를 띄어쓰기 형태 문자열로 변경
df['cate_mix'] = df.apply(lambda val:to_string(val.해시태그), axis=1) 
df['해시태그'] = df.apply(lambda val:to_list(val.해시태그), axis=1)
df.set_index('장소', inplace=True)
df.reset_index(inplace=True)
```

## 변환 된 데이터를 기반으로 매칭하기
변환 후, 사용자의 취향에 맞는 상위 음식점들과 해시태그 기반으로 만들어낸 유사도 모델과 비교해, 일치하는 장소가 있다면 출력하고, 없다면 가장 마지막에 나온 결과를 출력하도록 설계했다.

```python
def counter(arr): #사용자가 원하는 해시태그가 등장한 횟수를 반환하는 함수
    user_cok_list = ['카페','커피','음료'] #사용자가 원하는 해시태그라 가정
    count = 0
    for i in arr:
        if i in user_cok_list:
            count+=1
    return count
    
df['val_count'] = df.apply(lambda val:counter(val.해시태그), axis=1) #모든 데이터프레임에 해시태그 등장횟수를 적용
cok_arr = df.sort_values('val_count',ascending=False)['장소'].values #가장 많이 나온 순서대로 정렬

from sklearn.feature_extraction.text import CountVectorizer  # 피체 벡터화
from sklearn.metrics.pairwise import cosine_similarity  # 코사인 유사도

count_vect_category = CountVectorizer(min_df=0, ngram_range=(1,2))
place_category = count_vect_category.fit_transform(df['cate_mix'])
place_simi_cate = cosine_similarity(place_category, place_category)
place_simi_cate_sorted_ind = place_simi_cate.argsort()[:, ::-1]

def find_simi_place(df, sorted_ind, place_name, top_n):
    place_title = df[df['장소'] == place_name]
    place_index = place_title.index.values
    similar_indexes = sorted_ind[place_index, :(top_n)]
    similar_indexes = similar_indexes.reshape(-1)
    return df.iloc[similar_indexes]

for name in cok_arr[:50]: #상위 50개만 보도록하기
    n_list = find_simi_place(df, place_simi_cate_sorted_ind, name, 6) #자기 자신이 포함되는 결과를 낼 수 있으므로 6개를 가져온 뒤
    find_list = n_list.장소.values #이름 값만 가져와서
    
    if find_list[0] in l_data: #만약 가장 먼저 나온 음식점이 사용자의 취향과 가장 잘 맞는 상위 목록에 있을 때 반복문 종료
        break
    
n_list[:1] #break를 통해 나온 결과나, 반복문을 끝까지 돌린 후 나온 값이 출력됨 
```

# 6. 숙소에도 적용해보기
음식점에서 한 방식 그대로 숙소에도 적용해보기로 했다.

```python
l_data = final['숙박'][0]

df = pd.read_excel('./list/수정된파일.xlsx')
df.drop('Column1', axis=1, inplace=True)
df = df[df['타입'] == '숙박']

df['cate_mix'] = df.apply(lambda val:to_string(val.해시태그), axis=1)
df['해시태그'] = df.apply(lambda val:to_list(val.해시태그), axis=1)
df.set_index('장소', inplace=True)
df.reset_index(inplace=True)

def counter2(arr):
    user_cok_list = ['휴식 ', '애월 ', '게스트하우스']
    count = 0
    for i in arr:
        if i in user_cok_list:
            count+=1
    return count

df['val_count'] = df.apply(lambda val:counter2(val.해시태그), axis=1)
cok_arr = df.sort_values('val_count',ascending=False)['장소'].values
cok_arr

from sklearn.feature_extraction.text import CountVectorizer  # 피체 벡터화
from sklearn.metrics.pairwise import cosine_similarity  # 코사인 유사도

count_vect_category = CountVectorizer(min_df=0, ngram_range=(1,2))
place_category = count_vect_category.fit_transform(df['cate_mix'])
place_simi_cate = cosine_similarity(place_category, place_category)
place_simi_cate_sorted_ind = place_simi_cate.argsort()[:, ::-1]

def find_simi_place(df, sorted_ind, place_name, top_n):
    place_title = df[df['장소'] == place_name]
    place_index = place_title.index.values
    similar_indexes = sorted_ind[place_index, :(top_n)]
    similar_indexes = similar_indexes.reshape(-1)
    return df.iloc[similar_indexes]

for name in cok_arr[:50]:
    n_list = find_simi_place(df, place_simi_cate_sorted_ind, name, 6)
    find_list = n_list.장소.values
    
    if find_list[0] in l_data:
        break
        
n_list[:1]
```

변경점은 음식점 -> 숙소로 바뀐 것 뿐이다.

# 7. 정리
결과적으로 봤을 때, 제일 문제가 됐던 부분은 시간 분배였다. 데이터 수집 및 정제에 2주, 데이터 분석 1주, 시각화(UI제작) 1주 로 총 4주를 잡고 했는데, 생각보다 시간이 촉박해서 데이터 분석 및 시각화가 제대로 이뤄지지 않았다.    
머신러닝에 대한 깊은 이해가 없었기 때문에 중간중간 생기는 커다란 오류를 미리 인지하지 못했고, 촉박한 시간안에 주먹구구식으로 데이터 분석-출력을 하는데 너무 아쉬웠다. 

특히 성능평가를 수치화해서 나타냈어야 하는데, 그런 검증부분을 하지 못했던게 아쉬웠던 부분이다. 시간이 좀 더 된다면 추후에 이 부분을 보완해서 결과를 내보고 싶다.

# 8. 마치며
지금까지 포스트에 나온 소스의 원본은 다음의 깃허브 주소에서 확인 할 수 있습니다.

https://github.com/ponopono0322/Bori